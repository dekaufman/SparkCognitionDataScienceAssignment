---
title: "SparkCognition Data Science Test Project:  Targeting of Insurance Marketing"
output: html_notebook
---

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(gridExtra)        # Combine multiple graphs
library(knitr)
library(kableExtra)       # Table print formatting
library(GGally)           # Correlation plot using ggplot2
library(GoodmanKruskal)   # Categorical variable influence relationships
library(caTools)          # Training-validation split
library(randomForest)

setwd('C:/Users/David/Documents/Me/Energy/Jobs/2018/SparkCognition/SparkCognitionDataScienceAssignment')
source('helper functions.R')
```

### Ingest training data and report dimensions
```{r, message=FALSE, warning=FALSE}
df_train_orig <- read_csv('marketing_training.csv')
# %>% 
#   mutate_if(is.character, as.factor) 

dim(df_train_orig)
```
## Generate descriptive statistics on individual features
Separate the features into categorical and numeric, for separate descriptive methods
```{r}
df_train_numeric <- df_train_orig %>% select_if(is.numeric)
df_train_categ <- df_train_orig %>% select_if(is.character)
```

### Summarize categorical variables
```{r}
# Make tidy
df_cat_tidy <- df_train_categ %>% 
  gather('feature', 'value', 1:11)

# Count values
df_cat_dists <- df_cat_tidy %>% 
  group_by(feature, value) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  arrange(feature, desc(count))

# Compute distributions
df_cat_dists <- df_cat_dists %>% 
  group_by(feature) %>% 
  mutate(share = count / sum(count))
```

### Visualize categorical variables
```{r fig.width=10, fig.height=10}
fn_make_hist <- function(var_name, data_frame) {
  df_current_col <- data_frame %>% filter(feature == var_name)
  graph <- ggplot(data = df_current_col, 
                   aes(x = reorder(value, -share), y = share)) +
    geom_col() +
    stat_identity(geom="text", aes(label=count), vjust=-0.5, size = 3) +
    labs(x = var_name) +
    coord_cartesian(ylim = c(0, 1))
  if (nrow(df_current_col) > 4) {
    graph <- graph + theme(axis.text.x = element_text(angle = 45, hjust = 1))
  } 
  return(graph)
}

l_histograms <- lapply(colnames(df_train_categ), fn_make_hist, data_frame = df_cat_dists)

# Assemble set of histograms
grid.arrange(grobs = l_histograms, nrow = 4)
```

### Descriptive stats for numerical variables
```{r warning=FALSE, rows.print = 25, results = 'asis'}
# Per https://stackoverflow.com/a/34594642

# Compute quantiles, etc.
df_train_numer_stats <- df_train_numeric %>%
  summarise_all(funs(min = min, 
                     q25 = quantile(., 0.25), 
                     median = median, 
                     q75 = quantile(., 0.75), 
                     max = max,
                     mean = mean, 
                     sd = sd), 
                 na.rm = TRUE)

# Count NAs by column
df_train_numer_num_na <- df_train_numeric %>%
  summarise_all(funs(numNA = sum(is.na(.))))

df_train_numer_stats <- df_train_numer_stats %>% bind_cols(df_train_numer_num_na)

# Reshape back to tidy format
df_train_numer_stats <- df_train_numer_stats %>% 
  gather(stat, val) %>%
  separate(stat, into = c("var", "stat"), sep = "_") %>%
  spread(stat, val) %>%
  select(var, min, q25, median, q75, max, mean, sd, numNA) # reorder columns

# Formatting per http://haozhu233.github.io/kableExtra/awesome_table_in_html.html
kable(as.data.frame(df_train_numer_stats), digits = c(0, 1, 1, 1, 1, 1, 1, 1, 0)) %>% kable_styling()
```

### Visualize numerical variables:  violin plots, marked with quartiles
```{r fig.width=10, fig.height=10}
df_numer_tidy <- df_train_numeric %>% gather('feature', 'value')

fn_make_violin <- function(var_name, data_frame) {
  df_current_col <- data_frame %>% filter(feature == var_name)
  graph <- ggplot(data = df_current_col, 
                   aes(x = feature, y = value)) +
    geom_violin(na.rm = TRUE, draw_quantiles = c(0.25, 0.5, 0.75)) + 
    theme(axis.title.x = element_blank(), axis.title.y = element_blank())
  return(graph)
}

l_histograms <- lapply(colnames(df_train_numeric), fn_make_violin, data_frame = df_numer_tidy)

# # Assemble set of histograms
grid.arrange(grobs = l_histograms, nrow = 4)
```

## Multivariate descriptive statistics and correlations
```{r warning=FALSE,  fig.width=10, fig.height=10}
ggpairs(df_train_numeric)
```

```{r fig.width=10, fig.height=10}
# Per https://www.r-bloggers.com/to-eat-or-not-to-eat-thats-the-question-measuring-the-association-between-categorical-variables/
GKmatrix <- GKtauDataframe(as.data.frame(df_train_categ))
plot(GKmatrix, corrColors = "blue")
```

```{r eval=FALSE, include=FALSE}
# Chi-square evaluation of bivariate categorical feature association, per 
# https://stackoverflow.com/a/32748027
# Setting chunk header to suppress this; didn't turn out to be interesting
chisqmatrix <- function(x) {
  names = colnames(x);  num = length(names)
  m = matrix(nrow=num,ncol=num,dimnames=list(names,names))
  for (i in 1:(num-1)) {
    for (j in (i+1):num) {
      # https://stackoverflow.com/a/4740272 for unlist() fix
      m[i,j] = chisq.test(unlist(x[,i]),unlist(x[,j]))$p.value
    }
  }
  return (m)
}
mat <-  chisqmatrix(df_train_categ) %>% tbl_df()
kable(mat, digits = rep(4, 11)) %>% kable_styling()

```

# Removing abnormal records and undesired features
From the data exploration above, we draw the following conclusions:
* Features `pdays` and `pmonths` are effectively the same (correlation = 1, unsurprisingly given their definitions), so we will remove `pmonths`.
* There's a strong correlation among `euribor.3m`, `cons price idx`, and `nr.employed`.  Additionally, `nr.employed` has a small standard deviation relative to its mean.  Modeler's judgment: `nr.employed` is unlikely to be meaningful in prediction and will be removed.
* The categorical values that occur most rarely in their particular features (`default = 'yes'`, `marital = 'unknown'`, and `schooling == 'illiterate') are too unbalanced to generate reliable predictive impact, and will also tend to cause R library errors if they occur in the validation set but not the training set.  Therefore, the 10 affected rows will be discarded.
```{r}
df_train_orig <- df_train_orig %>% 
  select(-pmonths, -nr.employed) %>% 
  filter(default != 'yes' & marital != 'unknown' & 
           (schooling != 'illiterate' | is.na(schooling)))
```

## Extent of missing data
Count NAs per column, listing only the columns that have at least one NA.
```{r}
# Count NAs per column; show only the columns with at least 1
( df_incomplete <- df_train_orig %>% summarise_all(funs(sum(is.na(.)))) %>% select_if(function(col) sum(col) > 0) )
```

Tabulate rows by how many NAs they contain.
```{r}
# List of columns with NAs
l_cols_with_NAs <- colnames(df_incomplete)
# Count NAs per row
df_NA_per_row <- df_train_orig %>% 
  mutate_all(funs(ifelse(is.na(.), 1, 0))) %>% 
  transmute(num_NA = rowSums(.))
# Tabulate rows by how many NAs they contain
table(df_NA_per_row)

```

Conclusion:  nearly half the records contain at least one NA, so discarding incomplete records would be an extreme measure likely to weaken and/or bias the predictive model significantly.

Instead, we will treat the missing categorical data as a distinct category `'NA_text'` and use an imputation library routine to impute values for the `custAge` numerical feature.

# Split the data
Although the actual test data for this problem is found in a separate file, we also need to split the data for purposes of model selection.  We will split the "training" data into 70% for actual training and 30% for validation.
```{r}
set.seed(101)
sample = sample.split(df_train_orig$responded, SplitRatio = 0.7)
df_train <- subset(df_train_orig, sample == TRUE)
df_validation <- subset(df_train_orig, sample == FALSE)

```

# Balance the sample
We explore the `unbalanced` library which offers a collection of methods to oversample, undersample, or generate synthetic records and a function `ubRacing()` to recommend a method.
```{r}
library(unbalanced)
set.seed(875)
#configure sampling parameters
ubConf <- list(percOver=200, percUnder=200, k=2, perc=50, method="percPos", w=NULL)

# Prepare dataset in form required by 'unbalanced' package -- preliminary replacement of missing data
df_train_impute1 <- df_train %>% 
  mutate_if(is.character, replace_na, replace = 'NA_text') %>% 
  mutate(custAge = ifelse(is.na(custAge), median(custAge, na.rm = TRUE), custAge)) %>%
  mutate_if(is.character, as.factor)

# df_train_impute1 <- rfImpute(responded ~ ., df_train_impute1, ntree = 50)

# Save factor level definitions
df_factor_levels <- df_train_impute1 %>% slice(1)
  
# Convert factors to numeric (required by 'unbalanced' package)  
df_train_impute1 <- df_train_impute1 %>% mutate_if(is.factor, as.numeric)

rebalancing_exploration <- ubRacing(responded ~., df_train_impute1 %>% as.data.frame(),
                    "randomForest", positive = 2, ubConf=ubConf, ntree=50)
```

As this analysis evolved, `ubRacing()` generally recommended either SMOTE or NCL as the rebalancing method best suited to the training data.  SMOTE turns out to favor the positive results class more strongly, which (in the modeler's judgment) is better suited to this problem.  This may yield a lesser overall accuracy, but it seems to do a better job with with sensitivity (avoiding false negatives), which is probably a major objective for this business operation.
```{r}
# Manual override (modeler's judgment)
rebalancing_exploration$best = 'ubSMOTE'

if (rebalancing_exploration$best == 'ubNCL') {
  # Subtract one from response because this library requires (0,1) response vars
  rebal_result <- ubNCL(X = df_train_impute1 %>% select(-responded),
                          Y = df_train_impute1$responded - 1)
  # Add 1 to undo previous conversion of response var
  df_train_rebal <- bind_cols(rebal_result[['X']], (rebal_result[['Y']] + 1) %>% tbl_df() ) %>%
    rename(responded = value)
} else if (rebalancing_exploration$best == 'ubSMOTE') {
  rebal_result <- ubBalance(X = df_train_impute1 %>% select(-responded),
                          Y = as.factor(df_train_impute1$responded - 1),
                          type = 'ubSMOTE', perc = 20)
  df_train_rebal <- bind_cols(rebal_result[['X']], (as.numeric(rebal_result[['Y']])) %>% tbl_df()) %>%
    rename(responded = value)
} else {
  stop('Unimplemented unbalancing method')
}

# Get column indices of categorical factors
l_factor_names <- df_factor_levels %>% select_if(is.factor) %>% names() 
l_factor_indices <- which(names(df_train_rebal) %in% l_factor_names)

# Convert numeric factor levels back to actual factor data
for (i in l_factor_indices) {
  df_train_rebal[i] <- levels(pull(df_factor_levels, i))[ df_train_rebal[i] %>% unlist()]
  # }
}
print('Response counts in training data after rebalancing:')
table(df_train_rebal$responded)

```


# Random Forest model fitting and evaluation
To get a sense of the impact of our data cleaning process, we begin by applying a Random Forest to the dataset after eliminating unwanted features and records, but before replacing missing data and rebalancing.  The model-fitting function invoked with the instruction to ignore records with missing data, which considerably reduces the sample size.
```{r}
df_train_0 <- df_train %>% 
  mutate_if(is.character, as.factor) 

set.seed(101)
(
  model_RF_origdata <- randomForest(responded ~ ., data = df_train_0, 
                                    importance = TRUE, proximity = TRUE, 
                                    na.action = na.omit)
)
```

We also fit a model with missing data replaced but still without rebalancing.

```{r}
# Not rebalanced, but with NAs removed
df_train_00 <- df_train %>% 
  mutate_if(is.character, replace_na, replace = 'NA_text') %>% 
  mutate_if(is.character, as.factor) 
df_train_00 <- rfImpute(responded ~ ., df_train_00, tree = 50)

set.seed(1010)
(
  model_RF_origdata <- randomForest(x = df_train_00 %>% select(-responded), 
                       y = df_train_00 %>% pull(responded), 
                       xtest = df_validation_imputed %>% select(-responded), 
                       ytest = df_validation_imputed %>% pull(responded), 
                       importance = TRUE, 
                       proximity = TRUE,
                       ntree = 100)
)
```


```{r}
varImpPlot(model_RF_origdata,
           sort = T,
           main="Variable Importance",
           n.var=15)
fn_ROC_AUC((model_RF_origdata$test$votes %>% as.data.frame())$yes, df_validation_imputed$responded)
```

```{r}
df_train_1 <- df_train_rebal %>% 
  mutate_if(is.character, as.factor) 

set.seed(11111)
# qqq1 <- randomForest(responded ~ ., data = df_train_1, importance = TRUE, proximity = TRUE, na.action = na.fail)
# qqq1
(
  qqq1 <- randomForest(x = df_train_1 %>% select(-responded), 
                       y = df_train_1 %>% pull(responded), 
                       xtest = df_validation_imputed %>% select(-responded), 
                       ytest = df_validation_imputed %>% pull(responded), 
                       importance = TRUE, 
                       proximity = TRUE,
                       ntree = 100)
)

```

```{r}
varImpPlot(qqq1,
           sort = T,
           main="Variable Importance",
           n.var=15)
fn_ROC_AUC((qqq1$test$votes %>% as.data.frame())$yes, df_validation_imputed$responded)

```





# Appendix:  Reproducibility
```{r}
R.version
```

```{r}
Sys.info()
```

```{r}
date()
```

TODO:  add package versions
TODO:  add git repo status


```{r eval=FALSE, include=FALSE}
Standard R Notebook boilerplate inserted by RStudio:
  
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

# ```{r}
plot(cars)
# ```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

```

